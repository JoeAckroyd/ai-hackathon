````txt
# PROMPT FOR CLAUDE ‚Äì VOICE BROWSER ASSISTANT (CHROME EXTENSION + NODE SERVER)

You are going to scaffold a small project with:

- A **Chrome extension** that:
  - Injects a floating microphone button into every page.
  - Uses the **Web Speech API** for speech-to-text.
  - Uses **speechSynthesis** for text-to-speech.
  - On each utterance, sends page context + the transcript to a **local dev server**.
  - Receives a structured JSON response describing which action to take.
  - Executes actions like:
    - Navigating to Gmail inbox.
    - Describing whether the user is in the inbox.
    - Counting unread emails in Gmail by looking at the DOM.
- A **Node.js Express server** that:
  - Exposes a POST `/api/voice-command` endpoint.
  - Calls the **OpenAI Chat Completions API** (HTTP via `node-fetch`).
  - Uses a prompt to produce a **JSON-only** response:
    ```json
    {
      "type": "command",
      "action": "<string or 'none'>",
      "params": { ... },
      "speakText": "<string the extension should say aloud>"
    }
    ```
  - Relays that JSON back to the extension.

## ARCHITECTURE SUMMARY (FOR THE HUMAN + CLAUDE)

- The **extension** runs in the browser, inside the current page (via `content.js`).
- The content script:
  - Listens for voice input via Web Speech API.
  - Builds a payload `{ utterance, url, title, pageText }`.
  - Calls `http://localhost:3000/api/voice-command` (the dev server).
- The **dev server**:
  - Receives that payload.
  - Calls `https://api.openai.com/v1/chat/completions` with a carefully designed prompt.
  - Gets a model response whose `message.content` is **pure JSON** (we instruct the model to do that).
  - Parses that JSON and sends it back to the extension.
- The extension then:
  - Optionally speaks `speakText` out loud with `speechSynthesis`.
  - Runs `runCommand(action, params)` to manipulate the page (navigate to Gmail, describe context, count unread emails, etc.).

For counting unread emails in Gmail, we‚Äôll use **heuristic selectors** that are known to work on many current Gmail layouts (e.g. rows with class `.zA.zE` for unread). These may need adjustment if Gmail‚Äôs DOM is different, but the logic itself is valid and clearly marked as heuristic in comments.

---

# TASK FOR CLAUDE

Create a project with this structure:

voice-browser-assistant/
  extension/
    manifest.json
    content.js
  server/
    package.json
    index.js
    .env.example

I want you to **output all these files with full code** (no placeholders like ‚ÄúTODO‚Äù, and no explanations outside code comments). Put each file in its own fenced code block with a clear filename comment at the top, like:

```txt
// FILE: extension/manifest.json
...
````

and so on.

Below are the **precise requirements and code constraints** for each file.

---

## 1. FILE: extension/manifest.json

Create a **Manifest V3** file with:

* Name: `"Voice Browser Assistant"`
* Version: `"1.0.0"`
* Description: `"Control web pages with voice and an AI backend; demo includes Gmail inbox navigation and unread counting."`
* No background script is strictly required for now; we‚Äôll do everything from the content script.
* The content script should run on **all pages**.

It should look logically like this (you may format it, but keep the fields and values):

```json
{
  "manifest_version": 3,
  "name": "Voice Browser Assistant",
  "version": "1.0.0",
  "description": "Control web pages with voice and an AI backend; demo includes Gmail inbox navigation and unread counting.",
  "permissions": ["activeTab"],
  "host_permissions": ["<all_urls>"],
  "content_scripts": [
    {
      "matches": ["<all_urls>"],
      "js": ["content.js"],
      "run_at": "document_idle"
    }
  ]
}
```

Please emit it as a valid JSON file.

---

## 2. FILE: extension/content.js

This is the main content script. It should:

1. **Create a floating mic button** in the bottom-right corner.

   * Use inline `style` properties, e.g.:

     * `position: fixed`
     * `bottom: "20px"`
     * `right: "20px"`
     * `borderRadius: "50%"`
     * etc.
   * Button icon:

     * Default: `"üéôÔ∏è"`
     * While listening: `"üî¥"`

2. **Set up Web Speech API**:

   * Use:

     ```js
     const SpeechRecognition =
       window.SpeechRecognition || window.webkitSpeechRecognition;
     ```
   * If not available, show `console.warn` and disable the button (or show alert once).
   * Configure:

     * `recognition.lang = "en-US";`
     * `recognition.interimResults = false;`
     * `recognition.maxAlternatives = 1;`
   * When button is clicked:

     * If not currently listening:

       * Start recognition.
       * Set flag `listening = true`.
       * Change button text to `"üî¥"`.
     * If currently listening:

       * Stop recognition.
       * Set `listening = false`.
       * Change button text back to `"üéôÔ∏è"`.
   * On `recognition.result`, call `handleTranscript(transcript)`, where `transcript` is `event.results[0][0].transcript`.
   * On `recognition.end`, make sure `listening = false` and button shows `"üéôÔ∏è"` again.

3. **Set up text-to-speech via `speechSynthesis`**:

   Implement:

   ```js
   function speak(text) {
     if (!window.speechSynthesis) {
       console.warn("speechSynthesis not supported");
       return;
     }
     const utterance = new SpeechSynthesisUtterance(text);
     utterance.rate = 1;
     utterance.pitch = 1;
     window.speechSynthesis.speak(utterance);
   }
   ```

4. **Implement `handleTranscript(text)`**:

   * Collect basic page context:

     ```js
     const url = window.location.href;
     const title = document.title || "";
     const pageText = (document.body && document.body.innerText) ? document.body.innerText.slice(0, 2000) : "";
     ```

   * Call the local dev server:

     ```js
     fetch("http://localhost:3000/api/voice-command", {
       method: "POST",
       headers: { "Content-Type": "application/json" },
       body: JSON.stringify({ utterance: text, url, title, pageText })
     })
       .then(res => res.json())
       .then(data => {
         if (data && typeof data === "object") {
           const { action, params, speakText } = data;
           if (speakText) speak(speakText);
           if (action) runCommand(action, params || {});
         }
       })
       .catch(err => {
         console.error("Error calling /api/voice-command:", err);
         speak("Sorry, something went wrong talking to the server.");
       });
     ```

5. **Implement command handling logic via `runCommand(action, params)`**:

   Implement at least the following actions:

   ```js
   function runCommand(action, params = {}) {
     if (action === "navigateEmail") {
       // Navigate to Gmail inbox (demo)
       window.location.href = "https://mail.google.com/mail/u/0/#inbox";
     } else if (action === "describePageContext") {
       describePageContext();
     } else if (action === "countUnreadEmails") {
       countUnreadEmails();
     } else if (action === "none") {
       // Do nothing (e.g., small talk only)
     } else {
       console.log("Unknown action from server:", action, params);
     }
   }
   ```

6. **Implement `describePageContext()`**:

   * Detect if we are on Gmail and in the inbox view.

   * This should be heuristic but simple:

     ```js
     function isGmail() {
       return window.location.hostname.includes("mail.google.com");
     }

     function isInboxView() {
       if (!isGmail()) return false;
       const hash = window.location.hash || "";
       const looksLikeInbox = hash.includes("#inbox");

       const inboxLabels = Array.from(
         document.querySelectorAll("a[title*='Inbox'], a[aria-label*='Inbox']")
       );
       const hasInboxLabel = inboxLabels.length > 0;

       return looksLikeInbox || hasInboxLabel;
     }

     function describePageContext() {
       if (isInboxView()) {
         speak("You are in your email inbox.");
       } else if (isGmail()) {
         speak("You are in your email, but not in the main inbox.");
       } else {
         speak("You are not on your email page.");
       }
     }
     ```

   * It‚Äôs fine to mark in comments that this is heuristic and may need adjustment if Gmail‚Äôs UI changes.

7. **Implement `countUnreadEmails()` for Gmail**:

   * Only attempt this if on Gmail.

   * Use **heuristic selectors**:

     * First try: rows with class `.zA.zE` (commonly unread threads in many Gmail layouts).
     * If zero found, fall back to scanning `.zA` rows and checking `aria-label` for ‚Äúunread‚Äù.

   * Then speak the count.

   Here is the logic you should implement:

   ```js
   function countUnreadEmails() {
     if (!isGmail()) {
       speak("I can only count unread emails when you are on your Gmail page.");
       return;
     }

     // Heuristic for unread rows; this matches many current Gmail layouts.
     let unreadRows = document.querySelectorAll(".zA.zE");
     let unreadCount = unreadRows.length;

     if (unreadCount === 0) {
       // Fallback heuristic based on aria-label containing 'unread'
       const allRows = document.querySelectorAll(".zA, tr");
       unreadCount = Array.from(allRows).filter((row) => {
         const aria = row.getAttribute && row.getAttribute("aria-label");
         if (!aria) return false;
         return aria.toLowerCase().includes("unread");
       }).length;
     }

     if (unreadCount === 0) {
       speak("It looks like you have no unread emails in this view.");
     } else if (unreadCount === 1) {
       speak("You have one unread email.");
     } else {
       speak(`You have ${unreadCount} unread emails.`);
     }
   }
   ```

8. Wrap the whole content script in an IIFE to avoid leaking globals unnecessarily, e.g.:

   ```js
   (function () {
     // all code here
   })();
   ```

Please output the full `content.js` file with all of the above functionality, including:

* Button injection.
* Speech recognition setup.
* `speak`, `handleTranscript`, `runCommand`, `describePageContext`, `countUnreadEmails`.

---

## 3. FILE: server/package.json

Create a minimal `package.json` suitable for:

* Node.js (CommonJS, no `"type": "module"`).
* Dependencies:

  * `"express"`
  * `"cors"`
  * `"dotenv"`
  * `"node-fetch"` (v2 ‚Äì compatible with `require`).
* Scripts:

  * `"start": "node index.js"`

Example shape (you can fill version numbers, but keep the keys and script):

```json
{
  "name": "voice-browser-assistant-server",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "start": "node index.js"
  },
  "dependencies": {
    "express": "^4.19.0",
    "cors": "^2.8.5",
    "dotenv": "^16.4.0",
    "node-fetch": "^2.7.0"
  }
}
```

Use reasonably up-to-date versions, but keep `node-fetch` at `^2.x` to work with `require`.

---

## 4. FILE: server/.env.example

Create a tiny `.env.example` with:

```txt
OPENAI_API_KEY=your_openai_api_key_here
PORT=3000
```

This is just an example; no secrets.

---

## 5. FILE: server/index.js

Implement a Node.js Express server:

1. **Imports & setup**:

   ```js
   const express = require("express");
   const cors = require("cors");
   const fetch = require("node-fetch");
   require("dotenv").config();
   ```

2. Create app and middleware:

   ```js
   const app = express();
   const PORT = process.env.PORT || 3000;

   app.use(cors());           // allow all origins in dev
   app.use(express.json());   // parse JSON bodies
   ```

3. **Utility**: a function to call the OpenAI Chat Completions API.

   * Use `fetch("https://api.openai.com/v1/chat/completions", { ... })`.

   * Use a **model** like `"gpt-4o-mini"` or another current Chat Completions model.

   * Build messages:

     * `system` message explaining the JSON format and actions.
     * `user` message containing `{ utterance, url, title, pageText }`.

   * Instruct the model **very clearly**:

     * Respond with **only a JSON object**, no markdown, no extra text.

     * Shape:

       ```json
       {
         "type": "command",
         "action": "navigateEmail" | "describePageContext" | "countUnreadEmails" | "none",
         "params": {},
         "speakText": "..."
       }
       ```

     * Map these utterances:

       * ‚Äúgo to my email‚Äù, ‚Äúopen my inbox‚Äù ‚Üí `navigateEmail`
       * ‚Äúwhere am I‚Äù, ‚Äúam I in my inbox‚Äù ‚Üí `describePageContext`
       * ‚Äúhow many unread emails do I have‚Äù ‚Üí `countUnreadEmails`
       * Anything else ‚Üí `type: "command", action: "none", params: {}, speakText: "<short friendly reply>"`

   * Example (you can adjust string formatting, but keep semantics):

     ```js
     async function callOpenAI(payload) {
       const apiKey = process.env.OPENAI_API_KEY;
       if (!apiKey) {
         throw new Error("Missing OPENAI_API_KEY in environment");
       }

       const { utterance, url, title, pageText } = payload;

       const systemPrompt = `
     ```

You are a browser voice assistant that controls the page via a content script.
You MUST respond with a single JSON object, no extra text, no markdown.
Shape:
{
"type": "command",
"action": "<string>",
"params": { },
"speakText": "<string the extension should say aloud>"
}

Valid actions:

* "navigateEmail"       // navigate to Gmail inbox page
* "describePageContext" // ask the content script to verbally describe whether the user is in their inbox
* "countUnreadEmails"   // ask the content script to count unread emails in the current view
* "none"                // for small talk or when no browser action is needed

Rules:

* If user says something like "go to my email", "open my inbox", "go to gmail":

  * action: "navigateEmail", params: {}, speakText: "Opening your email inbox."
* If user says something like "where am I", "am I in my inbox":

  * action: "describePageContext", params: {}, speakText: "Let me check where you are."
* If user asks about unread emails, e.g. "how many unread emails do I have":

  * action: "countUnreadEmails", params: {}, speakText: "I'll count your unread emails."
* For casual chat or anything else that does not clearly match the above:

  * action: "none"
  * params: {}
  * speakText: a short, friendly spoken reply.

Do NOT wrap the JSON in backticks or markdown.
Return ONLY the JSON object.
`.trim();

```
   const userPrompt = `
```

User utterance: "${utterance}"

Page URL: ${url}
Page title: ${title}

Page text (truncated):
"""${pageText || ""}"""
`.trim();

````
   const response = await fetch("https://api.openai.com/v1/chat/completions", {
     method: "POST",
     headers: {
       "Content-Type": "application/json",
       "Authorization": `Bearer ${apiKey}`
     },
     body: JSON.stringify({
       model: "gpt-4o-mini",
       messages: [
         { role: "system", content: systemPrompt },
         { role: "user", content: userPrompt }
       ],
       temperature: 0
     })
   });

   if (!response.ok) {
     const errorText = await response.text();
     throw new Error("OpenAI API error: " + errorText);
   }

   const data = await response.json();
   const content = data.choices && data.choices[0] && data.choices[0].message && data.choices[0].message.content;

   if (!content) {
     throw new Error("No content in OpenAI response");
   }

   // Expect content to be pure JSON; parse it safely.
   let parsed;
   try {
     parsed = JSON.parse(content);
   } catch (err) {
     console.error("Failed to parse model JSON content:", content);
     // Fallback: safe default
     parsed = {
       type: "command",
       action: "none",
       params: {},
       speakText: "Sorry, I had trouble understanding that."
     };
   }

   return parsed;
 }
 ```
````

* Note: the instructions above align with the current Chat Completions API: POST to `/v1/chat/completions` with `model` and `messages`.

4. **Implement POST `/api/voice-command`**:

   * Accept JSON with keys: `utterance`, `url`, `title`, `pageText`.
   * Validate minimally (ensure `utterance` exists).
   * Call `callOpenAI` and return its result as JSON.
   * Handle errors gracefully.

   Example structure:

   ```js
   app.post("/api/voice-command", async (req, res) => {
     try {
       const { utterance, url, title, pageText } = req.body || {};

       if (!utterance || typeof utterance !== "string") {
         return res.status(400).json({ error: "Missing 'utterance' in request body" });
       }

       const result = await callOpenAI({ utterance, url, title, pageText });

       // Ensure we always send a sane object
       if (!result || typeof result !== "object") {
         return res.json({
           type: "command",
           action: "none",
           params: {},
           speakText: "Sorry, I did not get a valid response from the AI."
         });
       }

       return res.json(result);
     } catch (err) {
       console.error("Error in /api/voice-command:", err);
       return res.status(500).json({
         type: "command",
         action: "none",
         params: {},
         speakText: "Sorry, something went wrong on the server."
       });
     }
   });
   ```

5. **Start the server**:

   ```js
   app.listen(PORT, () => {
     console.log(`Voice command server listening on http://localhost:${PORT}`);
   });
   ```

Please output `server/index.js` as a complete file with all of the above logic wired together and no missing imports.

---

## FINAL OUTPUT FORMAT

Please output all the files as separate code blocks in this order:

1. `extension/manifest.json`
2. `extension/content.js`
3. `server/package.json`
4. `server/.env.example`
5. `server/index.js`

Do not include any extra commentary outside of code comments.

```
```
